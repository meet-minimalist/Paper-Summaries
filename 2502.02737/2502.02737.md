This paper introduces SmolLM2, a state-of-the-art small language model (1.7 billion parameters) developed through a data-centric, multi-stage training approach.

### 1. Problem Statement

The core problem addressed is the computational expense and deployment challenges associated with large language models (LLMs), which hinder their use in resource-constrained environments. While LLMs have driven significant AI breakthroughs, their inherent size (typically >10 billion parameters) leads to high training and inference costs. This paper aims to develop a performant "small" LM (around 1.7 billion parameters) that is computationally inexpensive and can run on a wider range of devices while still delivering satisfactory performance across various tasks.

### 2. Methodology

The authors developed SmolLM2, a 1.7 billion parameter model based on the Llama architecture (Touvron et al., 2023), initially with a 2048 sequence length, later extended to 8k tokens. The model was trained on an extensive ~11 trillion tokens of data using a multi-stage training process (four stages) guided by an "online" approach of monitoring evaluation metrics and adapting dataset mixtures.

Key aspects of the methodology include:
*   **Data Curation**: Careful evaluation of existing web, code, math, and instruction-following datasets. The authors created new specialized datasets:
    *   **FineMath**: A collection of up to 54B tokens of math data, filtered using a Llama-3.1-70B-Instruct classifier with 3-scale and 5-scale annotation prompts to focus on high-quality mathematical deduction and reasoning content (Section 3.3.2).
    *   **Stack-Edu**: A filtered variant of StarCoder2Data focusing on educational and well-documented code across 15 programming languages, classified using StarEncoder model with Llama3-70B-Instruct annotations (Section 3.4).
    *   **SmolTalk**: A new instruction-following dataset that combines selected existing datasets with new synthetic datasets, including Magpie-Ultra conversational data and task-specific data like Smol-Constraint, Smol-Rewrite, and Smol-Summarization, generated using Distilabel (Section 5.1).
*   **Multi-Stage Pretraining (Figure 2)**:
    *   **Stage 1 (0-6T tokens)**: Focused on English web data (60% FineWeb-Edu, 40% DCLM) and 10% StarCoder-Data (code).
    *   **Stage 2 (6-8T tokens)**: Introduced 5% math data (OpenWebMath) and increased code data (20%), maintaining English web at 75%.
    *   **Stage 3 (8-10T tokens)**: Integrated InfiMM-WebMath (bringing total math to 10%), replaced StarCoderData with Stack-Edu and Jupyter Notebooks, and adjusted English web ratio to 40/60 FineWeb-Edu/DCLM.
    *   **Stage 4 (10-11T tokens - Decay Phase)**: Introduced highest quality math datasets (FineMath4+, InfiWebMath-3+, AugGSM8K) totaling 14% math, expanded Stack-Edu to 24%, reduced English web to 58%, and added 4% Cosmopedia v2 (synthetic textbooks).
*   **Context Length Extension**: From 2k to 8k tokens using an intermediate checkpoint from Stage 4 and a specific long-context data mixture.
*   **Post-training**: Supervised fine-tuning (SFT) on the SmolTalk dataset and Direct Preference Optimization (DPO) using UltraFeedback for alignment.

### 3. Key Results & Analysis

The evaluation compares SmolLM2 (base and instruct variants) against similarly sized state-of-the-art models like Qwen2.5-1.5B and Llama3.2-1B on various benchmarks.

**Base Model Performance (Section 4.7):**
SmolLM2 demonstrates competitive results, outperforming Llama3.2-1B across most benchmarks. It shows strong performance on general knowledge and reasoning tasks (e.g., MMLU-Pro, HellaSwag, ARC) compared to Qwen2.5-1.5B, while Qwen2.5-1.5B shows superior performance on math and code benchmarks (GSM8K, MATH, HumanEval).

**Ablation Study Outcomes:**
*   **English Web Data**: A 60% FineWeb-Edu and 40% DCLM mixture achieved a balanced performance, leveraging FineWeb-Edu's strength in educational content and DCLM's in diverse, conversational styles (Table 1, Figure 4).
*   **Math Data**: FineMath4+ consistently outperformed existing datasets like OWM and InfiMM-WebMath on GSM8K and MATH, highlighting the importance of high-quality, reasoning-focused math content (Figure 1).
*   **Pretraining Progression**: MMLU (MCF) accuracy showed significant improvement during the stable phases, achieving above-random (>25%) accuracy after 6T tokens (Table 3, Figure 6).
*   **Context Length Extension**: No significant degradation in performance was observed after extending the context length to 8k, with strong results on HELMET and Needle in the Haystack benchmarks (Table 11, Figure 7).

**Instruct Model Performance (Section 5.4):**
The instruction-tuned SmolLM2-Instruct model shows strong instruction-following capabilities, significantly outperforming Qwen2.5-1.5B-Instruct on IFEval. It is competitive on MT-Bench and OpenRewrite-Eval and demonstrates strong mathematical capabilities, notably surpassing Qwen2.5-1.5B-Instruct on MATH after instruction tuning.

| Metric (Base Model) | SmolLM2-1.7B | Llama3.2-1B | Qwen2.5-1.5B |
| :------------------ | :----------- | :---------- | :------------ |
| HellaSwag           | **68.7**     | 61.2        | 66.4          |
| ARC                 | **60.5**     | 49.2        | 58.5          |
| PIQA                | **77.6**     | 74.8        | 76.1          |
| MMLU-Pro            | **19.4**     | 11.7        | 13.7          |
| GSM8K (5-shot)      | 31.1         | 7.6         | **61.7**      |
| MATH (4-shot)       | 11.6         | 3.3         | **34.3**      |
| HumanEval           | 22.6         | 18.9        | **37.2**      |

| Metric (Instruct Model) | SmolLM2-1.7B | Llama3.2-1B | Qwen2.5-1.5B |
| :---------------------- | :----------- | :---------- | :------------ |
| IFEval (Average)        | **56.7**     | 53.5        | 47.4          |
| OpenRewrite-Eval        | 44.9         | 39.2        | **46.9**      |
| GSM8K (5-shot)          | 48.8         | 37.4        | **63.3**      |
| MATH (4-shot)           | **21.0**     | 19.5        | 19.6          |
| HumanEval               | 28.1         | **33.5**    | 30.5          |

### 4. Core Contribution

The most significant contribution of this work is demonstrating that small language models can achieve state-of-the-art performance by leveraging a **data-centric, multi-stage training approach** with carefully curated and new high-quality, specialized datasets. This strategy optimizes the limited capacity of small models to learn core knowledge and fundamental capabilities, addressing limitations in existing public datasets and enabling strong performance across a variety of benchmarks including reasoning, mathematics, and instruction-following.

### 5. Open Source Contributions

The authors make significant open-source contributions to facilitate future research:
*   **SmolLM2**: Both the base and instruction-tuned variants of SmolLM2 (1.7B parameters), as well as smaller 360M and 135M parameter models, are released.
*   **Prepared Datasets**: All datasets prepared in the course of this project are released. This includes:
    *   **FineMath**: `https://huggingface.co/datasets/HuggingFaceTB/finemath`
    *   **Stack-Edu** (implicit release, as it's a filtered variant of StarCoder2Data)
    *   **SmolTalk**: `https://huggingface.co/datasets/HuggingFaceTB/smoltalk`
*   **Hugging Face Collection**: A collection for SmolLM2: `https://hf.co/collections/HuggingFaceTB/smollm2-6723884218bcda64b34d7db9`

### 6. Noteworthy Citations

1.  **Touvron et al., 2023. Llama:** This paper is foundational as SmolLM2's architecture is based on Llama.
2.  **Hoffmann et al., 2022. Training compute-optimal large language models:** This work established guidelines for compute-optimal training, which SmolLM2's extended training strategy intentionally deviates from to achieve performance gains for smaller models.
3.  **Radford et al., 2019. Language models are unsupervised multitask learners:** This paper describes the GPT-2 model, whose tokenizer is used in SmolLM2.
4.  **Penedo et al., 2024a. The fineweb datasets: Decanting the web for the finest text data at scale:** FineWeb-Edu, a key component of SmolLM2's web data mixture, builds upon the techniques described in this work.
5.  **Rafailov et al., 2024. Direct preference optimization: Your language model is secretly a reward model:** This paper describes the DPO method used for the final alignment stage of SmolLM2.