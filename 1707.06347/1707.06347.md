This paper introduces Proximal Policy Optimization (PPO) algorithms, a new family of policy gradient methods for reinforcement learning.

### 1. Problem Statement

The core problem addressed is the need for a reinforcement learning algorithm that is scalable to large models and parallel implementations, data-efficient, and robust (i.e., performs well across various problems without extensive hyperparameter tuning). Existing methods have significant limitations: deep Q-learning with function approximation often fails on simple problems and is poorly understood; "vanilla" policy gradient methods suffer from poor data efficiency and robustness, often leading to destructive policy updates if optimized for multiple steps; and Trust Region Policy Optimization (TRPO), while robust and data-efficient, is complex to implement and incompatible with architectures involving noise (e.g., dropout) or parameter sharing between policy and value functions. The paper aims to develop a first-order optimization algorithm that achieves TRPO's performance and stability but with greater simplicity and generality.

### 2. Methodology

The authors propose Proximal Policy Optimization (PPO), which alternates between sampling data from the environment and optimizing a "surrogate" objective function using stochastic gradient ascent over multiple epochs of minibatch updates. The key methodological contributions are:

1.  **Clipped Surrogate Objective ($L^{CLIP}$)**: This novel objective function (Equation 7) is designed to penalize policy changes that move the probability ratio $r_t(\theta) = \frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{old}}(a_t|s_t)}$ too far from 1, thereby preventing excessively large policy updates. It takes the minimum of two terms: the standard surrogate objective ($L^{CPI}$) and a clipped version of the probability ratio multiplied by the advantage estimate $\hat{A}_t$. The clipping operation restricts $r_t(\theta)$ to the interval $[1-\epsilon, 1+\epsilon]$, where $\epsilon$ is a hyperparameter (e.g., 0.2). By taking the minimum, the objective forms a pessimistic lower bound on the unclipped objective, ignoring beneficial policy changes outside the clip range but including detrimental ones. [Image 1] illustrates this behavior for positive and negative advantages.

2.  **Adaptive KL Penalty Coefficient**: As an alternative or addition to the clipping, the paper explores an objective function that includes a KL divergence penalty, $L^{KLPEN}(\theta)$ (Equation 8). The penalty coefficient $\beta$ is adaptively adjusted during training to maintain the KL divergence between the old and new policies close to a target value ($d_{targ}$). If the KL divergence is too low, $\beta$ is halved; if too high, $\beta$ is doubled.

The overall PPO algorithm (Algorithm 1) involves:
*   Running the current policy in the environment for `T` timesteps to collect data.
*   Computing advantage estimates (e.g., using Generalized Advantage Estimation, GAE).
*   Optimizing the surrogate loss (typically $L^{CLIP}$ combined with a value function loss and an entropy bonus, $L^{CLIP+VF+S}(\theta)$ in Equation 9) for `K` epochs using minibatch SGD (e.g., Adam) on the collected data.
*   Setting the old policy parameters ($\theta_{old}$) to the newly optimized parameters ($\theta$).

### 3. Key Results & Analysis

The experiments compare different PPO variants and other algorithms across continuous control (MuJoCo, Roboschool) and Atari environments.

**Comparison of Surrogate Objectives (Continuous Control - MuJoCo):**
An ablation study on 7 MuJoCo tasks with 3 random seeds per task (21 runs total) evaluated different objective functions. Scores were normalized (random policy = 0, best = 1).

| Algorithm Variant               | Avg. Normalized Score |
| :------------------------------ | :-------------------- |
| No clipping or penalty          | -0.39                 |
| Clipping, $\epsilon = 0.1$        | 0.76                  |
| **Clipping, $\epsilon = 0.2$**    | **0.82**              |
| Clipping, $\epsilon = 0.3$        | 0.70                  |
| Adaptive KL $d_{targ} = 0.003$  | 0.68                  |
| Adaptive KL $d_{targ} = 0.01$   | 0.74                  |
| Adaptive KL $d_{targ} = 0.03$   | 0.71                  |
| Fixed KL, $\beta = 0.3$         | 0.62                  |
| Fixed KL, $\beta = 1.$          | 0.71                  |
| Fixed KL, $\beta = 3.$          | 0.72                  |
| Fixed KL, $\beta = 10.$         | 0.69                  |
*Analysis*: The clipped surrogate objective with $\epsilon=0.2$ significantly outperformed other variants, including fixed and adaptive KL penalties, and the no-clipping baseline (which performed worse than a random policy in some cases). This highlights the effectiveness of the clipping mechanism in balancing policy update size and stability.

**Comparison to Other Algorithms (Continuous Control - MuJoCo):**
PPO (with clipped objective, $\epsilon=0.2$) was compared against TRPO, CEM, Vanilla Policy Gradient with adaptive stepsize, A2C, and A2C with trust region.
*Analysis*: PPO consistently outperformed these methods on almost all continuous control environments. [Image 3] shows the learning curves, illustrating PPO's superior performance over 1 million timesteps.

**Humanoid Running and Steering (Roboschool):**
PPO successfully trained policies for complex 3D humanoid control tasks (forward locomotion, flagrun, flagrun harder with perturbations).
*Analysis*: The learning curves demonstrate PPO's ability to handle high-dimensional continuous control problems effectively. [Image 4] shows performance over 50-100 million timesteps. [Image 5] provides still frames of a learned policy.

**Comparison to Other Algorithms (Atari Domain):**
PPO was compared against A2C and ACER on 49 Atari games.

| Scoring Metric                                  | A2C | ACER | PPO | Tie |
| :---------------------------------------------- | :-- | :--- | :-- | :-- |
| (1) Avg. episode reward over all of training    | 1   | 18   | **30** | 0   |
| (2) Avg. episode reward over last 100 episodes  | 1   | **28** | 19  | 1   |
*Analysis*: PPO performed significantly better than A2C in terms of sample complexity (measured by average reward over all training, metric 1), winning 30 games compared to A2C's 1 and ACER's 18. For final performance (average reward over last 100 episodes, metric 2), ACER slightly edged out PPO (28 wins vs 19). However, PPO is noted for being much simpler than ACER. [Image 6] and Table 6 in the appendix provide detailed results across all 49 games, showing PPO's strong overall performance.

### 4. Core Contribution

The single most significant and novel contribution of this work is the introduction of **Proximal Policy Optimization (PPO) with the clipped surrogate objective ($L^{CLIP}$)**. This method offers the stability and reliability of trust-region methods like TRPO but with significantly greater simplicity (using first-order optimization), generality (compatible with noise, parameter sharing), and empirically better sample complexity. By enabling multiple gradient updates per data sample without destructive policy changes, PPO strikes a favorable balance between performance, implementation ease, and data efficiency.

### 5. Open Source Contributions

The paper mentions an implementation of a vanilla policy gradient with adaptive stepsize (used for comparison in Section 6.2) available at:
*   [https://github.com/berkeleydeeprlcourse/homework/tree/master/hw4](https://github.com/berkeleydeeprlcourse/homework/tree/master/hw4)

The experiments were conducted using OpenAI Gym [Bro+16] environments, which are publicly available.

### 6. Noteworthy Citations

1.  **[Sch+15b] J. Schulman, S. Levine, P. Moritz, M. I. Jordan, and P. Abbeel. "Trust region policy optimization". In: CoRR, abs/1502.05477 (2015).**: This is the foundational paper for TRPO, against which PPO is primarily compared and aims to improve upon in terms of simplicity and generality.
2.  **[Mni+16] V. Mnih, A. P. Badia, M. Mirza, A. Graves, T. P. Lillicrap, T. Harley, D. Silver, and K. Kavukcuoglu. "Asynchronous methods for deep reinforcement learning". In: arXiv preprint arXiv:1602.01783 (2016).**: This paper introduces A3C (and its synchronous version, A2C), which is a key baseline for comparison, especially on Atari games.
3.  **[Wan+16] Z. Wang, V. Bapst, N. Heess, V. Mnih, R. Munos, K. Kavukcuoglu, and N. de Freitas. "Sample Efficient Actor-Critic with Experience Replay". In: arXiv preprint arXiv:1611.01224 (2016).**: This paper introduces ACER, another strong baseline algorithm PPO is compared against for its sample efficiency, particularly in the Atari domain.