# Attention Is All You Need

**Authors:** Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin

**arXiv ID:** 1706.03762

**Published:** June 12, 2017

**Link:** https://arxiv.org/abs/1706.03762

---

## Core Contribution

"Attention is All You Need" introduces the **Transformer**, a revolutionary sequence transduction model that completely eliminates recurrent and convolutional layers, relying solely on attention mechanisms. This architecture fundamentally changed how we approach sequence modeling by enabling greater parallelization and dramatically reducing training time while achieving superior performance.

The key innovation is a purely attention-based encoder-decoder structure that processes sequences in parallel rather than sequentially, breaking free from the computational constraints of RNNs and LSTMs.

---

## Technical Approach

The Transformer architecture consists of:

- **Multi-headed Self-Attention:** Allows the model to jointly attend to information from different representation subspaces at different positions, capturing various aspects of relationships between tokens.

- **Scaled Dot-Product Attention:** Computes attention weights efficiently using query, key, and value matrices with scaling to prevent gradient issues.

- **Position Embeddings:** Since the model has no recurrence or convolution, positional encodings are added to input embeddings to inject information about token positions in the sequence.

- **Encoder-Decoder Structure:** Stacked layers of self-attention and feed-forward networks in both encoder and decoder, with cross-attention in the decoder to attend to encoder outputs.

---

## Key Results & Ablations

**Machine Translation Performance:**
- **WMT 2014 English-to-German:** Achieved 28.4 BLEU, surpassing previous best results including ensembles by over 2 BLEU points
- **WMT 2014 English-to-French:** Established new single-model state-of-the-art BLEU score of 41.8 after training for only 3.5 days on eight GPUs

**Training Efficiency:**
- Significantly more parallelizable than RNN-based models
- Required a fraction of the training costs compared to best models in literature
- Faster convergence due to parallel processing capabilities

**Generalization:**
- Successfully applied to English constituency parsing with both large and limited training data, demonstrating versatility beyond machine translation

---

## Important Citations & Related Work

The paper builds upon and compares against:
- **RNN/LSTM-based sequence models:** Traditional recurrent architectures that process sequences sequentially
- **CNN-based sequence models:** Convolutional approaches to sequence transduction
- **Attention mechanisms:** Previous work on attention in encoder-decoder configurations, which the Transformer extends to be the sole computational mechanism

---

## Conclusion & Impact

The Transformer has become one of the most influential architectures in modern AI, serving as the foundation for:
- **BERT, GPT series, T5, and countless other models** that dominate NLP
- Extension to computer vision (Vision Transformers)
- Multi-modal models combining text, images, and other modalities

The paper's impact extends far beyond its original machine translation application, fundamentally reshaping deep learning research and establishing attention mechanisms as the dominant paradigm in sequence modeling and beyond. Its emphasis on parallelization and scalability paved the way for the large language models that define current AI capabilities.

---

*Summary generated on: 2025-11-20*