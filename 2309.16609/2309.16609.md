## QWEN Technical Report Summary

This report introduces QWEN, a comprehensive series of large language models (LLMs) developed by Alibaba Group, aiming to provide high-performing, reproducible, steerable, and accessible AI models to the community.

### 1. Problem Statement

The core problems addressed by this work are the lack of reproducibility, steerability, and accessibility often criticized in existing LLMs. While LLMs have demonstrated revolutionary capabilities in natural language processing and beyond, challenges remain in making these powerful models widely available and adaptable. Specifically, there's a need for comprehensive model series with varying parameter counts, refined through human alignment, and specialized for diverse tasks like coding, mathematics, and agent applications.

### 2. Methodology

The QWEN series encompasses base pretrained models (QWEN), chat models finetuned with human alignment techniques (QWEN-CHAT), and specialized models for coding (CODE-QWEN, CODE-QWEN-CHAT) and mathematics (MATH-QWEN-CHAT).

*   **Pretraining**: QWEN models are trained on massive datasets of up to 3 trillion tokens, including diverse texts and codes, with a significant multilingual component (English and Chinese). Data quality is ensured through comprehensive preprocessing, deduplication, filtering, and up-sampling high-quality instruction data.
    *   **Tokenization**: Uses a modified BPE tokenizer based on tiktoken, augmented with Chinese characters and other languages, resulting in a vocabulary size of approximately 152K.
    *   **Architecture**: A modified Transformer architecture based on LLaMA, incorporating untied embedding/output projection, Rotary Positional Embedding (ROPE) with FP32 precision for inverse frequency matrix, biases in QKV layers, RMSNorm instead of layer normalization, and SwiGLU activation function.
    *   **Training**: Autoregressive language modeling with a context length of 2048, Flash Attention, and AdamW optimizer.
    *   **Context Length Extension**: Employs training-free techniques like NTK-aware interpolation, LogN-Scaling, and layer-wise window attention to extend context length up to 8192 tokens during inference.
*   **Alignment (SFT & RLHF)**:
    *   **Supervised Finetuning (SFT)**: Finetuned on curated chat-style data annotated for helpfulness, task performance, tool use, agent capabilities, and safety. Uses the ChatML-style format to distinguish roles and content, applying loss masks for system and user inputs.
    *   **Reinforcement Learning from Human Feedback (RLHF)**:
        *   **Reward Model (RM)**: Pretrained with Preference Model Pretraining (PMP) on a vast dataset of comparison data, then finetuned with human feedback on QWEN model responses.
        *   **Proximal Policy Optimization (PPO)**: Used for policy training, involving sampling two responses per query, KL divergence coefficient, value loss clipping, and a pretrained gradient to mitigate alignment tax.
*   **Specialized Models**:
    *   **CODE-QWEN**: Continued pretraining on extensive code datasets (90 billion tokens) and finetuned for code generation, debugging, and interpretation, supporting context lengths up to 8192.
    *   **MATH-QWEN-CHAT**: Specialized for mathematics reasoning, trained using SFT on augmented math instructional datasets with a sequence length of 1024.
*   **Agent Capabilities**: Explores tool-use (ReAct prompting), Python code interpreter for math reasoning and data analysis, and functioning as a Hugging Face Agent. The self-instruct strategy is used for SFT to enhance agent capabilities.

### 3. Key Results & Analysis

QWEN models demonstrate strong performance across diverse benchmarks, often outperforming open-source models of similar sizes and approaching proprietary models.

**Performance Highlights:**

| Model | Params | MMLU (5-shot) | C-Eval (5-shot) | HumanEval (0-shot) | GSM8K (8-shot) | Tool Selection (Acc.â†‘) | Code Interpreter (All Exec. %) | MATH (4-shot) |
| :-------------------- | :----- | :------------ | :------------ | :----------------- | :------------- | :---------------------- | :-------------------------- | :---------- |
| **QWEN-14B** | 14B | 66.3 | 72.1 | 32.3 | 24.8 | - | - | 61.3 |
| **QWEN-14B-Chat (SFT)** | 14B | 66.5 | 71.7 | 43.9 | 59.3 | 98 | 81.7 | - |
| **QWEN-14B-Chat (RLHF)** | 14B | - | - | 45.8 (Human) | - | - | - | - |
| **CODE-QWEN-14B-Chat** | 14B | - | - | 66.4 | - | - | - | - |
| **MATH-QWEN-14B-Chat** | 14B | - | - | - | 69.8 | - | - | 24.2 |
| GPT-3.5 | - | 69.1 | 52.5 | 73.2 | 78.2 | 85 | 72.9 | 34.1 |
| GPT-4 | - | 83.0 | 69.9 | 86.6 | 91.4 | 95 | 86.8 | 42.5 |
| LLaMA 2 70B | 70B | 68.9 | 50.1 | 29.9 | 13.5 | - | - | 54.4 |
| Baichuan2 13B | 13B | 59.5 | 59.0 | 17.1 | 10.1 | - | - | 52.8 |

*   **General Performance**: QWEN-14B significantly outperforms previous SOTA models of similar sizes (e.g., LLaMA 2 13B and Baichuan2 13B) across a range of benchmarks (Table 2, Figure 2).
*   **Context Length**: Techniques like NTK-aware interpolation and LogN-Scaling effectively maintain low perplexity with context lengths exceeding 8192 tokens (Table 3).
*   **Alignment**: QWEN-CHAT models, especially the RLHF-trained versions, show superior performance in human evaluations, indicating improved human preference alignment (Figure 4). QWEN-14B-Chat (SFT) generally outperforms other open-source chat models on MMLU, C-Eval, GSM8K, HumanEval, and BBH (Table 5).
*   **Tool Use & Agents**: QWEN-CHAT models achieve high accuracy in tool selection (up to 98% for 7B/14B models) and tool input generation, with significantly lower false positive errors compared to GPT-3.5 (Table 6).
*   **Code Interpreter**: QWEN-CHAT (14B) demonstrates excellent executability (89.2%) and correctness (56.4% on Vis-All) in coding tasks, surpassing most open-source models like CODE LLAMA and InternLM-Chat (Table 7, Table 8). This capability often involves multi-step planning and understanding complex data structures like CSVs. [Figure 5] illustrates QWEN-CHAT's ability to plan and investigate CSV columns before plotting, contrasting with CODE LLAMA's initial errors.
*   **CODE-QWEN**: Specialized CODE-QWEN models (e.g., CODE-QWEN-CHAT 14B with 66.4% pass@1 on HumanEval) significantly outperform previous baselines and even larger generalist models, rivaling some larger proprietary models (Table 10, Table 11).
*   **MATH-QWEN**: MATH-QWEN-CHAT models show strong mathematical reasoning abilities, outperforming open-sourced models and approaching GPT-3.5 on GSM8K and MATH datasets (Table 12).
*   **Human Evaluation**: Examples illustrate QWEN-CHAT's improved factual accuracy and reasoning for knowledge, language understanding, and creative writing tasks compared to GPT-3.5. (See examples on pages 40-49).

### 4. Core Contribution

The most significant contribution of this work is the **release of QWEN, a comprehensive, high-performing, and open-source large language model series** (including base, chat, and specialized variants for coding and mathematics) that demonstrates competitive capabilities against proprietary models and significantly outperforms other open-source alternatives across a wide array of benchmarks and human evaluations, thereby fostering collaboration and innovation in the LLM community.

### 5. Open Source Contributions

The authors officially open-source the 14B-parameter and 7B-parameter base pretrained models QWEN and aligned chat models QWEN-CHAT.
*   **Models**: QWEN and QWEN-CHAT models (1.8B, 7B, 14B parameters).
*   **Code Interpreter Benchmark**: A publicly available benchmark for evaluating QWEN's ability to use a Python code interpreter for math problem-solving, data visualization, and general purposes.
*   **Tool Use Benchmark**: A publicly available benchmark for evaluating QWEN's ability to call plugins, tools, functions, or APIs using ReAct Prompting.
*   **GitHub Repository**: [https://github.com/QwenLM/Qwen](https://github.com/QwenLM/Qwen)
*   **Code Interpreter Benchmark Link**: [https://github.com/QwenLM/Qwen-Agent/tree/main/benchmark](https://github.com/QwenLM/Qwen-Agent/tree/main/benchmark)
*   **Tool Use Benchmark Link**: [https://github.com/QwenLM/Qwen-7B/tree/main/eval](https://github.com/QwenLM/Qwen-7B/tree/main/eval)

### 6. Noteworthy Citations

1.  **Touvron et al., 2023a; Touvron et al., 2023b (LLaMA & LLaMA 2)**: These papers are fundamental as QWEN's architecture is a modified version of the LLaMA family, and LLaMA models are a primary baseline for comparison.
2.  **OpenAI, 2023 (GPT-4 technical report) & Ouyang et al., 2022 (Training language models to follow instructions with human feedback)**: These represent the state-of-the-art proprietary models and the pioneering work on human alignment techniques (RLHF), which QWEN-CHAT models aim to compete with and build upon.
3.  **Yao et al., 2022 (ReAct: Synergizing reasoning and acting in language models)**: This paper introduced the ReAct prompting technique, which is a key methodology for QWEN's tool-use and agent capabilities.